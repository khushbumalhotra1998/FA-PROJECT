{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FA PROJECT- 2 \n",
        "\n",
        "**015049- KHUSHBU MALHOTRA **\n",
        "\n"
      ],
      "metadata": {
        "id": "YrzI9izC9R1W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E7ZS8ga9DpD"
      },
      "source": [
        "# Disaster Tweet Classification \n",
        "\n",
        "Following steps are performed: <br>\n",
        "<br>\n",
        "1) Pre-processing:<br>\n",
        "    i) Remove urls , non-ascii characters, emoji's,  punctuations, contractions (standard preprocessing <br>\n",
        "    most of the notebooks :) <br>\n",
        "    ii) to-lower <br>\n",
        "<br>   \n",
        "2) Class - Balancing : Sampling from class with more training example to balance class.<br>\n",
        "<br>\n",
        "3) Model, Optimizer and Other configurations:<br>\n",
        "tokenizer and bert model = 'bert-base-uncased' <br>\n",
        "optimizer= AdamW <br>\n",
        "scheduler linear with warmup <br>\n",
        "start_lr = 2e-5 <br>\n",
        "train_bs = 8 <br>\n",
        "valid_bs = 8 <br>\n",
        "epochs = 5 <br>\n",
        "max_len = 160 <br>\n",
        "dropout_ratio = 0.1 <br>\n",
        "warmup_epochs = 0 <br>\n",
        "test_size = 0.2 <br>\n",
        "<br>\n",
        "4) Ran for 3 different seed - 42, 11, 2020 and averaged the predictions.<br>\n",
        "<br>\n",
        "5)Repository also contains basic visualizations and tracking using 'weights and biases'.<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsLLtiHv9DpK"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "cYDTCUWA9DpN"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW3-ITY19DpP"
      },
      "outputs": [],
      "source": [
        "import config\n",
        "import dataset\n",
        "import model\n",
        "import train\n",
        "import util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quka2frW9DpQ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er247v3A9DpR"
      },
      "outputs": [],
      "source": [
        "warmup_epochs = 0\n",
        "epochs = 5\n",
        "model_name = 'bert-base-uncased'\n",
        "test_size = 0.3\n",
        "dropout = 0.1\n",
        "num_classes = 2\n",
        "linear_in = 768\n",
        "max_len = 160\n",
        "train_bs = 16\n",
        "valid_bs = 8\n",
        "start_lr = 2e-5\n",
        "\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyAdS6gF9DpT"
      },
      "outputs": [],
      "source": [
        "util.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd74EfU09DpU"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('C:\\\\Users\\\\user\\\\Khushbu\\\\train.csv')\n",
        "test_df = pd.read_csv('C:\\\\Users\\\\user\\\\Khushbu\\\\test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfuyTAmp9DpV"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At5fZ1vy9DpW"
      },
      "outputs": [],
      "source": [
        "import preprocess\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTdTfuO79DpX"
      },
      "outputs": [],
      "source": [
        "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
        "train_df = preprocess.fix_erraneous(train_df, ids_with_target_error, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsX0WblZ9DpX"
      },
      "outputs": [],
      "source": [
        "def remove_tabs_newlines(text):\n",
        "    text = re.sub(r'\\t', ' ', text) # remove tabs\n",
        "    text = re.sub(r'\\n', ' ', text) # remove line jump\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loPhHIht9DpY"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(lambda k : remove_tabs_newlines(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k : remove_tabs_newlines(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihp3pPeV9DpZ"
      },
      "outputs": [],
      "source": [
        "def remove_url(text):\n",
        "    return re.sub(r\"http\\S+\", \"URL\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob8rIsOL9DpZ"
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(text):\n",
        "    return ''.join([x for x in text if x in string.printable])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTUTcWqX9Dpa"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jtai6Xy9Dpb"
      },
      "outputs": [],
      "source": [
        "def remove_usrname(text):\n",
        "    text = re.sub(r'@\\S{0,}', ' USER ', text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r'\\b(USER)( \\1\\b)+', r'\\1', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOKeeU4k9Dpb"
      },
      "outputs": [],
      "source": [
        "\n",
        "contractions = { \n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there had\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"thx\"   : \"thanks\"\n",
        "}\n",
        "\n",
        "def remove_contractions(text):\n",
        "    return contractions[text.lower()] if text.lower() in contractions.keys() else text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn_Dipss9Dpd"
      },
      "outputs": [],
      "source": [
        "def remove_punctuations(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1vyHp289Dpd"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(lambda k: remove_usrname(k))\n",
        "train_df['text'] = train_df['text'].apply(lambda k: remove_contractions(k))\n",
        "train_df['text'] = train_df['text'].apply(lambda k: remove_url(k))\n",
        "train_df['text'] = train_df['text'].apply(lambda k: remove_emoji(k))\n",
        "train_df['text'] = train_df['text'].apply(lambda k: remove_non_ascii(k))\n",
        "train_df['text'] = train_df['text'].apply(lambda k: remove_punctuations(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifnei-5k9Dpe"
      },
      "outputs": [],
      "source": [
        "test_df['text'] = test_df['text'].apply(lambda k: remove_usrname(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: remove_contractions(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: remove_url(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: remove_emoji(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: remove_non_ascii(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: remove_punctuations(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TqAOKMx9Dpe"
      },
      "outputs": [],
      "source": [
        "def cleanup(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXlqTe0-9Dpf"
      },
      "outputs": [],
      "source": [
        "def replace_nums(text):\n",
        "    text = re.sub(r'^\\d\\S{0,}| \\d\\S{0,}| \\d\\S{0,}$', ' NUMBER ', text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r'\\b(NUMBER)( \\1\\b)+', r'\\1', text)\n",
        "    text = re.sub(r\"[0-9]\", \" \", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x33opuF9Dpf"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(lambda k: replace_nums(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: replace_nums(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy4w3aOx9Dpg"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(lambda k: cleanup(k))\n",
        "test_df['text'] = test_df['text'].apply(lambda k: cleanup(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHzt-qUv9Dpg",
        "outputId": "46c3e5a8-8d51-474a-aa2a-c72ff5daf3c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Our Deeds are the Reason of this earthquake Ma...\n",
              "1                Forest fire near La Ronge Sask Canada\n",
              "2    All residents asked to shelter in place are be...\n",
              "3    NUMBER people receive wildfires evacuation ord...\n",
              "4    Just got sent this photo from Ruby Alaska as s...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['text'][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7JrwxWx9Dpi"
      },
      "source": [
        "# Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpMsSwtI9Dpj"
      },
      "outputs": [],
      "source": [
        "train_y = train_df['target']\n",
        "train_df.drop(['target'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLMGBqhD9Dpj"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test \\\n",
        "        = train_test_split(train_df['text'], train_y, random_state=SEED, test_size=test_size, stratify=train_y.values) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUeyBWTF9Dpk"
      },
      "source": [
        "# **Encode and get dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fbb710a1eec84d9c81ad28d1e72823a3"
          ]
        },
        "id": "kx9O3NXi9Dpk",
        "outputId": "071e1bdc-6aac-46d6-d874-5aef1c69267d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbb710a1eec84d9c81ad28d1e72823a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        do_lower_case=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuHaqeN69Dpk"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.BertDataset(\n",
        "        text=X_train.values,\n",
        "        tokenizer= tokenizer,\n",
        "        max_len= max_len,\n",
        "        target=y_train.values\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvJtVcVi9Dpl"
      },
      "outputs": [],
      "source": [
        "valid_dataset = dataset.BertDataset(\n",
        "        text=X_test.values,\n",
        "        tokenizer= tokenizer,\n",
        "        max_len= max_len,\n",
        "        target=y_test.values\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl_ERUiZ9Dpl"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        train_bs,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICRU8vqU9Dpl"
      },
      "outputs": [],
      "source": [
        "valid_dl = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        valid_bs,\n",
        "        shuffle=True,\n",
        "        num_workers=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6MoZZXK9Dpm"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6ee3218bef0a4a7b84c4378fbc856fe8",
            "0358e35d79c7428f9d29f6feb3577ccf"
          ]
        },
        "id": "zw2YDz3R9Dpm",
        "outputId": "9dca3ad1-b82a-49dd-90e8-e70a68a1ae4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ee3218bef0a4a7b84c4378fbc856fe8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0358e35d79c7428f9d29f6feb3577ccf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bert = model.BertHf(model_name, dp=dropout, num_classes=num_classes, linear_in=linear_in)\n",
        "bert = bert.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTJXKy_C9Dpn"
      },
      "outputs": [],
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEtipB-19Dpn"
      },
      "outputs": [],
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
        "params = list(bert.named_parameters())\n",
        "modified_params = [\n",
        "    {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n",
        "    {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-flA9BsP9Dpn"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.AdamW(modified_params, lr=start_lr, eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysMFN0lg9Dpo"
      },
      "outputs": [],
      "source": [
        "total_steps = int(len(train_df) * epochs / train_bs)\n",
        "warmup_steps = int(len(train_df) * warmup_epochs / train_bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMBYjJCW9Dpo"
      },
      "outputs": [],
      "source": [
        "sched = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFxz6b_09Dpo"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3A5qKh_9Dpp"
      },
      "source": [
        "# Now Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4EPcgyl9Dpp"
      },
      "outputs": [],
      "source": [
        "train_stat = util.AvgStats()\n",
        "valid_stat = util.AvgStats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j1aECrZ9Dpp"
      },
      "outputs": [],
      "source": [
        "best_acc = 0.0\n",
        "best_model_file = str(model_name) + '_best.pth.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyzgsa969Dpq",
        "outputId": "194e4477-42a7-49fb-ea7c-469bc9d25603"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/334 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch\tTrain Acc\tTrain Loss\tTrain F1\tValid Acc\tValid Loss\tValid F1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/334 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1\t79.63970726\t0.45451723\t0.73987053\t83.45008757\t0.39817595\t0.79501085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/334 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2\t88.00900732\t0.31460791\t0.85031623\t82.66199650\t0.41558999\t0.78524946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/334 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3\t92.08106587\t0.22391058\t0.90280977\t82.96847636\t0.46835563\t0.78400888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/334 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4\t94.78326140\t0.14512766\t0.93699003\t81.43607706\t0.62212932\t0.78693467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5\t96.26571589\t0.10047031\t0.95547102\t82.00525394\t0.67934453\t0.78627145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"\\nEpoch\\tTrain Acc\\tTrain Loss\\tTrain F1\\tValid Acc\\tValid Loss\\tValid F1\")\n",
        "for i in range(epochs):\n",
        "    start = time.time()\n",
        "    losses, ops, targs = train.train(train_dl, bert, criterion, optim, sched, device)\n",
        "    duration = time.time() - start\n",
        "    train_acc = accuracy_score(targs, ops)\n",
        "    train_f1_score = f1_score(targs, ops)\n",
        "    train_loss = sum(losses)/len(losses)\n",
        "    train_prec = precision_score(targs, ops)\n",
        "    train_rec = recall_score(targs, ops)\n",
        "    train_roc_auc = roc_auc_score(targs, ops)\n",
        "    train_stat.append(train_loss, train_acc, train_f1_score, train_prec, train_rec, train_roc_auc, duration)\n",
        "    start = time.time()\n",
        "    lossesv, opsv, targsv = train.test(valid_dl, bert, criterion, device)\n",
        "    duration = time.time() - start\n",
        "    valid_acc = accuracy_score(targsv, opsv)\n",
        "    valid_f1_score = f1_score(targsv, opsv)\n",
        "    valid_loss = sum(lossesv)/len(lossesv)\n",
        "    valid_prec = precision_score(targsv, opsv)\n",
        "    valid_rec = recall_score(targsv, opsv)\n",
        "    valid_roc_auc = roc_auc_score(targsv, opsv)\n",
        "    valid_stat.append(valid_loss, valid_acc, valid_f1_score, valid_prec, valid_rec, valid_roc_auc, duration)\n",
        "\n",
        "    if valid_acc > best_acc:\n",
        "        best_acc = valid_acc\n",
        "        util.save_checkpoint(bert, True, best_model_file)\n",
        "        tfpr, ttpr, _ = roc_curve(targs, ops)\n",
        "        train_stat.update_best(ttpr, tfpr, train_acc, i)\n",
        "        vfpr, vtpr, _ = roc_curve(targsv, opsv)\n",
        "        valid_stat.update_best(vtpr, vfpr, best_acc, i)\n",
        "\n",
        "    print(\"\\n{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\".format(i+1, train_acc*100, train_loss, \n",
        "                                                train_f1_score, valid_acc*100, \n",
        "                                                valid_loss, valid_f1_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAjaEASM9Dpq",
        "outputId": "852d498b-a4bb-4546-c490-77482fcd888b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of best run::\n",
            "Best Accuracy: 0.8345008756567426\n",
            "Roc Auc score: 0.8238655096288859\n",
            "Loss: 0.3981759548187256\n",
            "Area Under Curve: 0.8238655096288859\n"
          ]
        }
      ],
      "source": [
        "print(\"Summary of best run::\")\n",
        "print(\"Best Accuracy: {}\".format(valid_stat.best_acc))\n",
        "print(\"Roc Auc score: {}\".format(valid_stat.roc_aucs[valid_stat.best_epoch]))\n",
        "print(\"Loss: {}\".format(valid_stat.losses[valid_stat.best_epoch]))\n",
        "print(\"Area Under Curve: {}\".format(auc(valid_stat.fprs, valid_stat.tprs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9LzQp69Dpr"
      },
      "source": [
        "# Now make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9nheP8-9Dpr"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset.BertDataset(\n",
        "        text=test_df.text.values,\n",
        "        tokenizer= tokenizer,\n",
        "        max_len= max_len\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKoNtzlo9Dps"
      },
      "outputs": [],
      "source": [
        "test_dl = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        1,\n",
        "        shuffle=False,\n",
        "        num_workers=1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op2WFcvo9Dps"
      },
      "outputs": [],
      "source": [
        "util.load_checkpoint(bert, best_model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEEjd6be9Dpt",
        "outputId": "779fef07-bb18-446f-a441-f946f80caebf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \r"
          ]
        }
      ],
      "source": [
        "_, opst, _ = train.test(test_dl, bert, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pydJPOkY9Dpt"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeV-Vjmf9Dpt"
      },
      "outputs": [],
      "source": [
        "sub_csv = pd.DataFrame(columns=['id', 'target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H61tkC6w9Dpu"
      },
      "outputs": [],
      "source": [
        "sub_csv['id'] = test_df['id']\n",
        "sub_csv['target'] = opst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c3XZlhy9Dpu"
      },
      "outputs": [],
      "source": [
        "sub_csv.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "name": "015049_FA PROJECT 2 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}